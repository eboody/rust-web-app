/*
 * AnythingLLM Developer API
 *
 * API endpoints that enable programmatic reading, writing, and updating of your AnythingLLM instance. UI supplied by Swagger.io.
 *
 * The version of the OpenAPI document: 1.0.0
 *
 * Generated by: https://openapi-generator.tech
 */

use super::{configuration, Error};
use crate::{apis::ResponseContent, models};
use async_trait::async_trait;
use reqwest;
use serde::{Deserialize, Serialize};
use std::sync::Arc;

#[async_trait]
pub trait OpenAiCompatibleEndpointsApi: Send + Sync {
	async fn v1_openai_chat_completions_post(
		&self,
	) -> Result<(), Error<V1OpenaiChatCompletionsPostError>>;
	async fn v1_openai_embeddings_post(
		&self,
	) -> Result<(), Error<V1OpenaiEmbeddingsPostError>>;
	async fn v1_openai_models_get(
		&self,
	) -> Result<serde_json::Value, Error<V1OpenaiModelsGetError>>;
	async fn v1_openai_vector_stores_get(
		&self,
	) -> Result<serde_json::Value, Error<V1OpenaiVectorStoresGetError>>;
}

pub struct OpenAiCompatibleEndpointsApiClient {
	configuration: Arc<configuration::Configuration>,
}

impl OpenAiCompatibleEndpointsApiClient {
	pub fn new(configuration: Arc<configuration::Configuration>) -> Self {
		Self { configuration }
	}
}

#[async_trait]
impl OpenAiCompatibleEndpointsApi for OpenAiCompatibleEndpointsApiClient {
	/// Execute a chat with a workspace with OpenAI compatibility. Supports streaming as well. Model must be a workspace slug from /models.
	async fn v1_openai_chat_completions_post(
		&self,
	) -> Result<(), Error<V1OpenaiChatCompletionsPostError>> {
		let local_var_configuration = &self.configuration;

		let local_var_client = &local_var_configuration.client;

		let local_var_uri_str = format!(
			"{}/v1/openai/chat/completions",
			local_var_configuration.base_path
		);
		let mut local_var_req_builder = local_var_client
			.request(reqwest::Method::POST, local_var_uri_str.as_str());

		if let Some(ref local_var_user_agent) = local_var_configuration.user_agent {
			local_var_req_builder = local_var_req_builder
				.header(reqwest::header::USER_AGENT, local_var_user_agent.clone());
		}
		if let Some(ref local_var_token) =
			local_var_configuration.bearer_access_token
		{
			local_var_req_builder =
				local_var_req_builder.bearer_auth(local_var_token.to_owned());
		};

		let local_var_req = local_var_req_builder.build()?;
		let local_var_resp = local_var_client.execute(local_var_req).await?;

		let local_var_status = local_var_resp.status();
		let local_var_content = local_var_resp.text().await?;

		if !local_var_status.is_client_error() && !local_var_status.is_server_error()
		{
			Ok(())
		} else {
			let local_var_entity: Option<V1OpenaiChatCompletionsPostError> =
				serde_json::from_str(&local_var_content).ok();
			let local_var_error = ResponseContent {
				status: local_var_status,
				content: local_var_content,
				entity: local_var_entity,
			};
			Err(Error::ResponseError(local_var_error))
		}
	}

	/// Get the embeddings of any arbitrary text string. This will use the embedder provider set in the system. Please ensure the token length of each string fits within the context of your embedder model.
	async fn v1_openai_embeddings_post(
		&self,
	) -> Result<(), Error<V1OpenaiEmbeddingsPostError>> {
		let local_var_configuration = &self.configuration;

		let local_var_client = &local_var_configuration.client;

		let local_var_uri_str =
			format!("{}/v1/openai/embeddings", local_var_configuration.base_path);
		let mut local_var_req_builder = local_var_client
			.request(reqwest::Method::POST, local_var_uri_str.as_str());

		if let Some(ref local_var_user_agent) = local_var_configuration.user_agent {
			local_var_req_builder = local_var_req_builder
				.header(reqwest::header::USER_AGENT, local_var_user_agent.clone());
		}
		if let Some(ref local_var_token) =
			local_var_configuration.bearer_access_token
		{
			local_var_req_builder =
				local_var_req_builder.bearer_auth(local_var_token.to_owned());
		};

		let local_var_req = local_var_req_builder.build()?;
		let local_var_resp = local_var_client.execute(local_var_req).await?;

		let local_var_status = local_var_resp.status();
		let local_var_content = local_var_resp.text().await?;

		if !local_var_status.is_client_error() && !local_var_status.is_server_error()
		{
			Ok(())
		} else {
			let local_var_entity: Option<V1OpenaiEmbeddingsPostError> =
				serde_json::from_str(&local_var_content).ok();
			let local_var_error = ResponseContent {
				status: local_var_status,
				content: local_var_content,
				entity: local_var_entity,
			};
			Err(Error::ResponseError(local_var_error))
		}
	}

	/// Get all available \"models\" which are workspaces you can use for chatting.
	async fn v1_openai_models_get(
		&self,
	) -> Result<serde_json::Value, Error<V1OpenaiModelsGetError>> {
		let local_var_configuration = &self.configuration;

		let local_var_client = &local_var_configuration.client;

		let local_var_uri_str =
			format!("{}/v1/openai/models", local_var_configuration.base_path);
		let mut local_var_req_builder = local_var_client
			.request(reqwest::Method::GET, local_var_uri_str.as_str());

		if let Some(ref local_var_user_agent) = local_var_configuration.user_agent {
			local_var_req_builder = local_var_req_builder
				.header(reqwest::header::USER_AGENT, local_var_user_agent.clone());
		}
		if let Some(ref local_var_token) =
			local_var_configuration.bearer_access_token
		{
			local_var_req_builder =
				local_var_req_builder.bearer_auth(local_var_token.to_owned());
		};

		let local_var_req = local_var_req_builder.build()?;
		let local_var_resp = local_var_client.execute(local_var_req).await?;

		let local_var_status = local_var_resp.status();
		let local_var_content = local_var_resp.text().await?;

		if !local_var_status.is_client_error() && !local_var_status.is_server_error()
		{
			serde_json::from_str(&local_var_content).map_err(Error::from)
		} else {
			let local_var_entity: Option<V1OpenaiModelsGetError> =
				serde_json::from_str(&local_var_content).ok();
			let local_var_error = ResponseContent {
				status: local_var_status,
				content: local_var_content,
				entity: local_var_entity,
			};
			Err(Error::ResponseError(local_var_error))
		}
	}

	/// List all the vector database collections connected to AnythingLLM. These are essentially workspaces but return their unique vector db identifier - this is the same as the workspace slug.
	async fn v1_openai_vector_stores_get(
		&self,
	) -> Result<serde_json::Value, Error<V1OpenaiVectorStoresGetError>> {
		let local_var_configuration = &self.configuration;

		let local_var_client = &local_var_configuration.client;

		let local_var_uri_str = format!(
			"{}/v1/openai/vector_stores",
			local_var_configuration.base_path
		);
		let mut local_var_req_builder = local_var_client
			.request(reqwest::Method::GET, local_var_uri_str.as_str());

		if let Some(ref local_var_user_agent) = local_var_configuration.user_agent {
			local_var_req_builder = local_var_req_builder
				.header(reqwest::header::USER_AGENT, local_var_user_agent.clone());
		}
		if let Some(ref local_var_token) =
			local_var_configuration.bearer_access_token
		{
			local_var_req_builder =
				local_var_req_builder.bearer_auth(local_var_token.to_owned());
		};

		let local_var_req = local_var_req_builder.build()?;
		let local_var_resp = local_var_client.execute(local_var_req).await?;

		let local_var_status = local_var_resp.status();
		let local_var_content = local_var_resp.text().await?;

		if !local_var_status.is_client_error() && !local_var_status.is_server_error()
		{
			serde_json::from_str(&local_var_content).map_err(Error::from)
		} else {
			let local_var_entity: Option<V1OpenaiVectorStoresGetError> =
				serde_json::from_str(&local_var_content).ok();
			let local_var_error = ResponseContent {
				status: local_var_status,
				content: local_var_content,
				entity: local_var_entity,
			};
			Err(Error::ResponseError(local_var_error))
		}
	}
}

/// struct for typed errors of method [`v1_openai_chat_completions_post`]
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum V1OpenaiChatCompletionsPostError {
	Status400(),
	Status401(),
	Status403(models::InvalidApiKey),
	Status500(),
	UnknownValue(serde_json::Value),
}

/// struct for typed errors of method [`v1_openai_embeddings_post`]
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum V1OpenaiEmbeddingsPostError {
	Status403(models::InvalidApiKey),
	Status500(),
	UnknownValue(serde_json::Value),
}

/// struct for typed errors of method [`v1_openai_models_get`]
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum V1OpenaiModelsGetError {
	Status403(models::InvalidApiKey),
	Status500(),
	UnknownValue(serde_json::Value),
}

/// struct for typed errors of method [`v1_openai_vector_stores_get`]
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum V1OpenaiVectorStoresGetError {
	Status403(models::InvalidApiKey),
	Status500(),
	UnknownValue(serde_json::Value),
}
